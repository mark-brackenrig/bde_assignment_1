{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from pyspark.sql.types import DateType, IntegerType, BooleanType, TimestampType, FloatType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import lit\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Working Directory to Parent folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work/notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Download Dataset\n",
    "\n",
    "First step is to download the dataset from S3 - to do this, I used the boto3 library to access the public S3 data. I will be working with the 2017-18 data in this dataset.\n",
    "\n",
    "I have developed a function that will check my data folder for the required CSVs and download missing CSVs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_save_data(bucket = 'nyc-tlc',prefix = 'trip data',year_regex = '201[78]', data_folder = 'data'):\n",
    "    \"\"\"\n",
    "    This function downloads and saves relevant data from the S3 bucket to my local machine.\n",
    "    \n",
    "    params:\n",
    "    * s3 - a boto3.client('s3') object with relevant permissions\n",
    "    * bucket - bucket of stored data\n",
    "    * prefix - folder with relevant stored data\n",
    "    * year_regex - the years for the project\n",
    "    * data_folder - where I want to store the data\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    print(\"Starting yellow cabs download\")\n",
    "    sys.stdout.write(\"[%s]\" % (\" \" * 12))\n",
    "    sys.stdout.flush()\n",
    "    sys.stdout.write(\"\\b\" * (12+1)) # return to start of line, after '['\n",
    "    contents = s3.list_objects(Bucket = bucket, Prefix = prefix)['Contents']\n",
    "    #retrieve yellow cab keys and download file\n",
    "    yellow_cab_keys = [i['Key'] for i in contents if ('yellow_tripdata' in i['Key'])&(bool(re.search(year_regex,i['Key'])))]\n",
    "    for i in yellow_cab_keys:\n",
    "        if (re.sub(\".*/\",\"\",i) in os.listdir(data_folder+'/yellow_cabs/'))==False:\n",
    "            s3.download_file('nyc-tlc',i,data_folder+'/yellow_cabs/'+re.sub(\".*/\",\"\",i)) \n",
    "        sys.stdout.write(\"-\")\n",
    "        sys.stdout.flush()\n",
    "    sys.stdout.write(\"]\\n\")\n",
    "    print('Yellow Cabs completed')\n",
    "    print(\"Starting green cabs download\")\n",
    "    sys.stdout.write(\"[%s]\" % (\" \" * 12))\n",
    "    sys.stdout.flush()\n",
    "    sys.stdout.write(\"\\b\" * (12+1)) # return to start of line, after '['\n",
    "    green_cab_keys = [i['Key'] for i in contents if ('green_tripdata' in i['Key'])&(bool(re.search(year_regex,i['Key'])))]\n",
    "    for i in green_cab_keys:\n",
    "        if (re.sub(\".*/\",\"\",i) in os.listdir(data_folder+'/green_cabs/'))==False:\n",
    "            s3.download_file('nyc-tlc',i,data_folder+'/green_cabs/'+re.sub(\".*/\",\"\",i)) \n",
    "        sys.stdout.write(\"-\")\n",
    "        sys.stdout.flush()\n",
    "    sys.stdout.write(\"]\\n\")\n",
    "    print(\"Green Cabs completed\")\n",
    "    return \"Data downloaded and saved\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting yellow cabs download\n",
      "[            ------------------------]\n",
      "Yellow Cabs completed\n",
      "Starting green cabs download\n",
      "[            ------------------------]\n",
      "Green Cabs completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Data downloaded and saved'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_and_save_data(bucket = 'nyc-tlc',prefix = 'trip data',year_regex = '201[78]', data_folder = 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName('assignment_1') \\\n",
    "        .getOrCreate() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.5'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark._jvm.org.apache.hadoop.util.VersionInfo.getVersion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = [\"data/green_cabs/\" + i for i in  os.listdir('data/green_cabs')]\n",
    "y = [\"data/yellow_cabs/\" + i for i in  os.listdir('data/yellow_cabs')]\n",
    "paths = [i for y in [g,y] for i in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "green = spark.read.format('csv').options(header ='true').load(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow = spark.read.format('csv').options(header ='true').load(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Cleaning & Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Create a column for 'taxi_colour'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "green = green.withColumn('taxi_colour', lit('green'))\n",
    "yellow = yellow.withColumn('taxi_colour', lit('yellow'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Create timestamp columns with same name in green and yellow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "green = green.withColumn('pickup_date', F.col('lpep_pickup_datetime').astype(TimestampType())).\\\n",
    "    withColumn(\"dropoff_date\", F.col(\"lpep_dropoff_datetime\").astype(TimestampType()))\n",
    "yellow = yellow.withColumn('pickup_date', F.col('tpep_pickup_datetime').astype(TimestampType()) ).\\\n",
    "    withColumn(\"dropoff_date\", F.col(\"tpep_dropoff_datetime\").astype(TimestampType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii. Remove columns\n",
    "\n",
    "Some columns have been removed from the dataset. Namely:\n",
    "1. ehail_fee:  This is not in the data dictionary, but assume it is the fee for a cab to be 'hailed'. It does not exist in the yellow cabs dataset so therefore it should be excluded\n",
    "2. Trip_type: only exists in the green taxi dataset - refers to whether a cab was dispatched or hailed\n",
    "3. date/time stamps: these were transformed into new columns and had names aligned.\n",
    "4. fair_amount: removed as per assignment brief\n",
    "\n",
    "I have decided to leave 'ID' fields as strings for now, as they do not represent numbers but rather a categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols =['VendorID',\n",
    " 'store_and_fwd_flag',\n",
    " 'RatecodeID',\n",
    " 'PULocationID',\n",
    " 'DOLocationID',\n",
    " 'passenger_count',\n",
    " 'trip_distance',\n",
    " 'extra',\n",
    " 'mta_tax',\n",
    " 'tip_amount',\n",
    " 'tolls_amount',\n",
    " 'improvement_surcharge',\n",
    " 'total_amount',\n",
    " 'payment_type',\n",
    " 'taxi_colour',\n",
    " 'pickup_date',\n",
    " 'dropoff_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "green = green.select(cols)\n",
    "yellow = yellow.select(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iv. Merge datasets\n",
    "Merge the datasets into 1 dataframe, and view top 5 records and schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = green.union(yellow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+----------+------------+------------+---------------+-------------+-----+-------+----------+------------+---------------------+------------+------------+-----------+-------------------+-------------------+\n",
      "|VendorID|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|payment_type|taxi_colour|        pickup_date|       dropoff_date|\n",
      "+--------+------------------+----------+------------+------------+---------------+-------------+-----+-------+----------+------------+---------------------+------------+------------+-----------+-------------------+-------------------+\n",
      "|       2|                 N|         1|         264|         264|              1|          .00|  0.2|      0|         0|           0|                    0|         1.7|           1|      green|2017-03-01 00:30:18|2017-03-01 00:30:47|\n",
      "|       2|                 N|         1|          95|          49|              1|        18.77|  0.5|    0.5|         0|           0|                  0.3|        58.8|           2|      green|2017-03-01 00:11:58|2017-03-01 01:05:54|\n",
      "|       2|                 N|         1|          92|         192|              1|         1.76|  0.5|    0.5|         0|           0|                  0.3|         9.8|           2|      green|2017-03-01 00:54:44|2017-03-01 01:03:42|\n",
      "|       2|                 N|         1|          70|          70|              2|         1.15|  0.5|    0.5|         0|           0|                  0.3|         6.8|           2|      green|2017-03-01 00:00:07|2017-03-01 00:04:39|\n",
      "|       2|                 N|         1|         196|         193|              1|         5.19|  0.5|    0.5|         0|           0|                  0.3|        19.8|           1|      green|2017-03-01 00:17:05|2017-03-01 00:36:48|\n",
      "+--------+------------------+----------+------------+------------+---------------+-------------+-----+-------+----------+------------+---------------------+------------+------------+-----------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- RatecodeID: string (nullable = true)\n",
      " |-- PULocationID: string (nullable = true)\n",
      " |-- DOLocationID: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- extra: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- taxi_colour: string (nullable = false)\n",
      " |-- pickup_date: timestamp (nullable = true)\n",
      " |-- dropoff_date: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v. Convert DataTypes\n",
    "Numeric datatypes need to be changed to a float or integer, categorical variables can remain as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark.read.parquet('data/combined_cleaned_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- store_and_fwd_flag: boolean (nullable = true)\n",
      " |-- RatecodeID: string (nullable = true)\n",
      " |-- PULocationID: string (nullable = true)\n",
      " |-- DOLocationID: string (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: float (nullable = true)\n",
      " |-- extra: float (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: float (nullable = true)\n",
      " |-- tolls_amount: float (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- taxi_colour: string (nullable = true)\n",
      " |-- pickup_date: timestamp (nullable = true)\n",
      " |-- dropoff_date: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs= dfs.withColumn('passenger_count', F.col('passenger_count').astype(IntegerType())).\\\n",
    "    withColumn('extra', F.col('extra').astype(FloatType())).\\\n",
    "    withColumn('mta_tax', F.col('mta_tax').astype(FloatType())).\\\n",
    "    withColumn('improvement_surcharge', F.col('improvement_surcharge').astype(FloatType())).\\\n",
    "    withColumn('total_amount', F.col('total_amount').astype(FloatType())).\\\n",
    "    withColumn('trip_distance', F.col('trip_distance').astype(FloatType())).\\\n",
    "    withColumn('tip_amount', F.col('tip_amount').astype(FloatType())).\\\n",
    "    withColumn('tolls_amount', F.col('tolls_amount').astype(FloatType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vi. Create Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = dfs.withColumn('trip_duration',F.col(\"dropoff_date\").cast(\"long\") - F.col('pickup_date').cast(\"long\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+----------+------------+------------+---------------+-------------+-----+-------+----------+------------+---------------------+------------+------------+-----------+-------------------+-------------------+-------------+\n",
      "|VendorID|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|payment_type|taxi_colour|        pickup_date|       dropoff_date|trip_duration|\n",
      "+--------+------------------+----------+------------+------------+---------------+-------------+-----+-------+----------+------------+---------------------+------------+------------+-----------+-------------------+-------------------+-------------+\n",
      "|       2|             false|         1|          66|          33|              5|         0.51|  0.5|    0.5|       0.7|         0.0|                  0.3|         6.0|           1|      green|2018-06-01 00:33:55|2018-06-01 00:36:13|          138|\n",
      "|       2|             false|         1|          25|          49|              5|         1.97|  0.5|    0.5|      2.06|         0.0|                  0.3|       12.36|           1|      green|2018-06-01 00:40:36|2018-06-01 00:49:46|          550|\n",
      "|       2|             false|         1|          61|          49|              5|          1.4|  0.5|    0.5|       0.0|         0.0|                  0.3|         7.8|           2|      green|2018-06-01 00:57:12|2018-06-01 01:02:58|          346|\n",
      "|       2|             false|         1|          49|          97|              1|         1.36|  0.5|    0.5|       0.0|         0.0|                  0.3|         8.3|           2|      green|2018-06-01 00:10:13|2018-06-01 00:16:27|          374|\n",
      "|       1|             false|         1|          75|         127|              1|          7.9|  0.5|    0.5|       6.3|         0.0|                  0.3|        31.6|           1|      green|2018-06-01 00:32:08|2018-06-01 00:52:06|         1198|\n",
      "+--------+------------------+----------+------------+------------+---------------+-------------+-----+-------+----------+------------+---------------------+------------+------------+-----------+-------------------+-------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Save Data In Optimized Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfs.write.mode('overwrite').parquet('data/combined_data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Remove data that appears errornous\n",
    "I decided to remove data that falls outside of the specified year ranges and where the trip duration is negative (i.e. drop off was before pick up)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark.read.parquet('combined_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|    trip_duration|\n",
      "+-------+-----------------+\n",
      "|  count|        236847300|\n",
      "|   mean|1018.441442178146|\n",
      "| stddev|206405.6298083972|\n",
      "|    min|      -2911502804|\n",
      "|    max|         45466304|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs.select(\"trip_duration\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|  year(pickup_date)|\n",
      "+-------+-------------------+\n",
      "|  count|          236847300|\n",
      "|   mean|  2017.471231265883|\n",
      "| stddev|0.49917167113572786|\n",
      "|    min|               2017|\n",
      "|    max|               2018|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs.select(year(\"pickup_date\")).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_cleaned = dfs.where(\"YEAR(pickup_date) <2019 AND YEAR(pickup_date) >2016 AND trip_duration > 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_cleaned.write.mode('overwrite').parquet('data/combined_clean_data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check values after subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|  year(pickup_date)|\n",
      "+-------+-------------------+\n",
      "|  count|          236638069|\n",
      "|   mean| 2017.4712503253227|\n",
      "| stddev|0.49917276919436876|\n",
      "|    min|               2017|\n",
      "|    max|               2018|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs_cleaned.select(year(\"pickup_date\")).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. SQL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from pyspark.sql.types import DateType, IntegerType, BooleanType, TimestampType, FloatType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import lit\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work/notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName('assignment_1') \\\n",
    "        .getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark.read.parquet('data/combined_clean_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+----------+------------+------------+---------------+-------------+-----+-------+----------+------------+---------------------+------------+------------+-----------+-------------------+-------------------+-------------+\n",
      "|VendorID|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|payment_type|taxi_colour|        pickup_date|       dropoff_date|trip_duration|\n",
      "+--------+------------------+----------+------------+------------+---------------+-------------+-----+-------+----------+------------+---------------------+------------+------------+-----------+-------------------+-------------------+-------------+\n",
      "|       2|             false|         1|         244|         116|              1|         1.77|  0.0|    0.5|       0.0|         0.0|                  0.3|         8.8|           2|     yellow|2017-03-13 09:09:07|2017-03-13 09:17:28|          501|\n",
      "|       1|             false|         1|         264|         264|              1|          1.5|  0.0|    0.5|       1.0|         0.0|                  0.3|        11.8|           1|     yellow|2017-03-13 09:09:08|2017-03-13 09:22:45|          817|\n",
      "|       1|             false|         1|         162|          88|              1|          5.7|  0.0|    0.5|       0.0|         0.0|                  0.3|        21.8|           1|     yellow|2017-03-13 09:09:08|2017-03-13 09:28:58|         1190|\n",
      "|       1|             false|         1|         233|         170|              1|          0.9|  0.0|    0.5|      1.75|         0.0|                  0.3|       10.55|           1|     yellow|2017-03-13 09:09:08|2017-03-13 09:20:03|          655|\n",
      "|       2|             false|         1|         164|         246|              5|         1.12|  0.0|    0.5|       0.0|         0.0|                  0.3|        11.3|           2|     yellow|2017-03-13 09:09:08|2017-03-13 09:24:19|          911|\n",
      "+--------+------------------+----------+------------+------------+---------------+-------------+-----+-------+----------+------------+---------------------+------------+------------+-----------+-------------------+-------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. For each year and month:\n",
    "### i. what was the total number of trips?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.createOrReplaceTempView(\"combined_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT COUNT(*) as trips, MONTH(pickup_date) as month, YEAR(pickup_date) as year\n",
      "FROM combined_data\n",
      "GROUP BY MONTH(pickup_date), YEAR(pickup_date)\n",
      "ORDER BY YEAR(pickup_date) desc,  MONTH(pickup_date) desc\n"
     ]
    }
   ],
   "source": [
    "query = open('sql/4a_i.sql','r').read()\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+----+\n",
      "|   trips|month|year|\n",
      "+--------+-----+----+\n",
      "| 8850453|   12|2018|\n",
      "| 8794310|   11|2018|\n",
      "| 9520702|   10|2018|\n",
      "| 8699790|    9|2018|\n",
      "| 8508476|    8|2018|\n",
      "| 8527273|    7|2018|\n",
      "| 9445404|    6|2018|\n",
      "|10013372|    5|2018|\n",
      "|10097508|    4|2018|\n",
      "|10258208|    3|2018|\n",
      "| 9254724|    2|2018|\n",
      "| 9545547|    1|2018|\n",
      "|10405588|   12|2017|\n",
      "|10148489|   11|2017|\n",
      "|10684481|   10|2017|\n",
      "| 9819538|    9|2017|\n",
      "| 9281644|    8|2017|\n",
      "| 9495191|    7|2017|\n",
      "|10623800|    6|2017|\n",
      "|11151727|    5|2017|\n",
      "|11116768|    4|2017|\n",
      "|11442309|    3|2017|\n",
      "|10182399|    2|2017|\n",
      "|10770368|    1|2017|\n",
      "+--------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query).show(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Which day of week had the most trips?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.createOrReplaceTempView(\"combined_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select b1.trips,b2.week_day, b1.month,b1.year from\n",
      "(select max(b.trips) as trips, b.month, b.year  from \n",
      "    (SELECT COUNT(*) as trips, DAYOFWEEK(pickup_date) as week_day,MONTH(pickup_date) as month, YEAR(pickup_date) as year\n",
      "FROM combined_data\n",
      "GROUP BY  DAYOFWEEK(pickup_date),MONTH(pickup_date), YEAR(pickup_date)) as b\n",
      "Group by b.month, b.year) as b1\n",
      "inner join (SELECT COUNT(*) as trips,  DAYOFWEEK(pickup_date) as week_day,MONTH(pickup_date) as month, YEAR(pickup_date) as year\n",
      "FROM combined_data\n",
      "GROUP BY  DAYOFWEEK(pickup_date),MONTH(pickup_date), YEAR(pickup_date)) as b2 \n",
      "on (b1.trips, b1.month,b1.year)=(b2.trips, b2.month,b2.year)\n",
      "ORDER BY b2.year desc,  b2.month desc\n"
     ]
    }
   ],
   "source": [
    "query = open('sql/4a_ii.sql','r').read()\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----+----+\n",
      "|  trips|week_day|month|year|\n",
      "+-------+--------+-----+----+\n",
      "|1507495|       7|   12|2018|\n",
      "|1523623|       6|   11|2018|\n",
      "|1574898|       4|   10|2018|\n",
      "|1471635|       7|    9|2018|\n",
      "|1487246|       4|    8|2018|\n",
      "|1455733|       3|    7|2018|\n",
      "|1643980|       6|    6|2018|\n",
      "|1743709|       5|    5|2018|\n",
      "|1522737|       2|    4|2018|\n",
      "|1810353|       6|    3|2018|\n",
      "|1463629|       6|    2|2018|\n",
      "|1626790|       4|    1|2018|\n",
      "|1829711|       6|   12|2017|\n",
      "|1742118|       4|   11|2017|\n",
      "|1675164|       3|   10|2017|\n",
      "|1723433|       6|    9|2017|\n",
      "|1605374|       5|    8|2017|\n",
      "|1528712|       7|    7|2017|\n",
      "|1854007|       5|    6|2017|\n",
      "|1859731|       4|    5|2017|\n",
      "|1967438|       7|    4|2017|\n",
      "|2032711|       6|    3|2017|\n",
      "|1615027|       7|    2|2017|\n",
      "|1700391|       3|    1|2017|\n",
      "+-------+--------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query).show(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii. Which hour of the day had the most trips? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.createOrReplaceTempView(\"combined_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select b1.trips,b2.hour, b1.month,b1.year from\n",
      "(select max(b.trips) as trips, b.month, b.year  from \n",
      "    (SELECT COUNT(*) as trips, HOUR(pickup_date) as hour,MONTH(pickup_date) as month, YEAR(pickup_date) as year\n",
      "FROM combined_data\n",
      "GROUP BY HOUR(pickup_date),MONTH(pickup_date), YEAR(pickup_date)) as b\n",
      "Group by b.month, b.year) as b1\n",
      "inner join (SELECT COUNT(*) as trips, HOUR(pickup_date) as hour,MONTH(pickup_date) as month, YEAR(pickup_date) as year\n",
      "FROM combined_data\n",
      "GROUP BY  HOUR(pickup_date),MONTH(pickup_date), YEAR(pickup_date)) as b2 \n",
      "on (b1.trips, b1.month,b1.year)=(b2.trips, b2.month,b2.year)\n",
      "ORDER BY b2.year desc,  b2.month desc\n"
     ]
    }
   ],
   "source": [
    "query = open('sql/4a_iii.sql','r').read()\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----+----+\n",
      "| trips|hour|month|year|\n",
      "+------+----+-----+----+\n",
      "|551064|  18|   12|2018|\n",
      "|555083|  18|   11|2018|\n",
      "|612201|  18|   10|2018|\n",
      "|556887|  18|    9|2018|\n",
      "|553112|  18|    8|2018|\n",
      "|547322|  18|    7|2018|\n",
      "|591334|  18|    6|2018|\n",
      "|636495|  18|    5|2018|\n",
      "|660835|  18|    4|2018|\n",
      "|666998|  18|    3|2018|\n",
      "|613828|  18|    2|2018|\n",
      "|632430|  18|    1|2018|\n",
      "|646656|  18|   12|2017|\n",
      "|650151|  18|   11|2017|\n",
      "|683472|  18|   10|2017|\n",
      "|621732|  19|    9|2017|\n",
      "|587847|  18|    8|2017|\n",
      "|588692|  18|    7|2017|\n",
      "|656305|  18|    6|2017|\n",
      "|689546|  18|    5|2017|\n",
      "|696864|  18|    4|2017|\n",
      "|734461|  19|    3|2017|\n",
      "|672653|  18|    2|2017|\n",
      "|692103|  18|    1|2017|\n",
      "+------+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query).show(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iv. What was the average number of passengers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.createOrReplaceTempView(\"combined_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT AVG(passenger_count) as passenger_count, MONTH(pickup_date) as month, YEAR(pickup_date) as year\n",
      "FROM combined_data\n",
      "GROUP BY  MONTH(pickup_date), YEAR(pickup_date)\n",
      "ORDER BY YEAR(pickup_date) desc, MONTH(pickup_date) desc\n"
     ]
    }
   ],
   "source": [
    "query = open('sql/4a_iv.sql','r').read()\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+----+\n",
      "|   passenger_count|month|year|\n",
      "+------------------+-----+----+\n",
      "|1.5752000490822333|   12|2018|\n",
      "|  1.55926013524654|   11|2018|\n",
      "|1.5531575297703888|   10|2018|\n",
      "| 1.570099852984957|    9|2018|\n",
      "|1.5824966774308349|    8|2018|\n",
      "|1.5861990111023772|    7|2018|\n",
      "|1.5790617320339078|    6|2018|\n",
      "|1.5780397452526482|    5|2018|\n",
      "|   1.5819909229089|    4|2018|\n",
      "|1.5820507831387314|    3|2018|\n",
      "|1.5766915361279277|    2|2018|\n",
      "|1.5868474588203274|    1|2018|\n",
      "|1.6096649223474926|   12|2017|\n",
      "|1.5907292208721908|   11|2017|\n",
      "|1.5959072789777997|   10|2017|\n",
      "|1.6035457065291667|    9|2017|\n",
      "| 1.609642429724734|    8|2017|\n",
      "|1.6154271146309749|    7|2017|\n",
      "|1.5996005195880947|    6|2017|\n",
      "|1.5955439906303301|    5|2017|\n",
      "| 1.601958860704838|    4|2017|\n",
      "|1.5927111389842732|    3|2017|\n",
      "|1.5990340783149433|    2|2017|\n",
      "|1.6034515255189052|    1|2017|\n",
      "+------------------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query).show(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v. What was the average amount paid per trip (total_amount)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.createOrReplaceTempView(\"combined_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT AVG(total_amount), MONTH(pickup_date) as month, YEAR(pickup_date) as year\n",
      "FROM combined_data\n",
      "GROUP BY  MONTH(pickup_date), YEAR(pickup_date)\n",
      "ORDER BY YEAR(pickup_date) desc, MONTH(pickup_date) desc\n"
     ]
    }
   ],
   "source": [
    "query = open('sql/4a_v.sql','r').read()\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+----+\n",
      "| avg(total_amount)|month|year|\n",
      "+------------------+-----+----+\n",
      "|16.436609998199245|   12|2018|\n",
      "| 16.78761144761138|   11|2018|\n",
      "| 16.90458035477482|   10|2018|\n",
      "|16.805398872795774|    9|2018|\n",
      "| 16.57291976715495|    8|2018|\n",
      "|  16.5406384863263|    7|2018|\n",
      "|  16.6268418697513|    6|2018|\n",
      "|16.731315664714753|    5|2018|\n",
      "|16.239193330053897|    4|2018|\n",
      "|15.877975423904495|    3|2018|\n",
      "|15.365707331375857|    2|2018|\n",
      "|15.366687762922156|    1|2018|\n",
      "| 16.01037110281087|   12|2017|\n",
      "|16.304495679901407|   11|2017|\n",
      "|16.554461719969563|   10|2017|\n",
      "| 16.49286644970195|    9|2017|\n",
      "| 16.28606673788002|    8|2017|\n",
      "|16.189206986182032|    7|2017|\n",
      "|16.449241718116834|    6|2017|\n",
      "|16.537219583692274|    5|2017|\n",
      "|16.084471557084566|    4|2017|\n",
      "|15.980580262136435|    3|2017|\n",
      "|15.447472305861448|    2|2017|\n",
      "|15.281115687377275|    1|2017|\n",
      "+------------------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query).show(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vi. What was the average amount paid per passenger (total_amount)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.createOrReplaceTempView(\"combined_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT AVG(total_amount/passenger_count) as cost_per_passenger, MONTH(pickup_date) as month, YEAR(pickup_date) as year\n",
      "FROM combined_data\n",
      "GROUP BY   MONTH(pickup_date), YEAR(pickup_date)\n",
      "ORDER BY YEAR(pickup_date) desc,  MONTH(pickup_date) desc\n"
     ]
    }
   ],
   "source": [
    "query = open('sql/4a_vi.sql','r').read()\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+----+\n",
      "|cost_per_passenger|month|year|\n",
      "+------------------+-----+----+\n",
      "|13.513153185236861|   12|2018|\n",
      "|13.918583215039412|   11|2018|\n",
      "|14.042497138982277|   10|2018|\n",
      "|13.911523668408037|    9|2018|\n",
      "| 13.67558963521544|    8|2018|\n",
      "|13.637959361794742|    7|2018|\n",
      "| 13.73353095947967|    6|2018|\n",
      "|13.833440853044157|    5|2018|\n",
      "|13.408293335529835|    4|2018|\n",
      "|13.118159156422765|    3|2018|\n",
      "|12.744442427910286|    2|2018|\n",
      "|12.704552575643287|    1|2018|\n",
      "|13.084454042445142|   12|2017|\n",
      "|13.451115974509026|   11|2017|\n",
      "|13.656228678419502|   10|2017|\n",
      "|13.568315565331623|    9|2017|\n",
      "|13.376032087675778|    8|2017|\n",
      "|13.265769164522586|    7|2017|\n",
      "| 13.58484491714844|    6|2017|\n",
      "| 13.63129327961534|    5|2017|\n",
      "| 13.24615287436987|    4|2017|\n",
      "|13.226031480897179|    3|2017|\n",
      "|12.745868074725589|    2|2017|\n",
      "|12.628996085969037|    1|2017|\n",
      "+------------------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query).show(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. For each taxi colour (yellow and green)\n",
    "### i. What was the average, median, minimum and maximum trip duration in seconds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.createOrReplaceTempView(\"combined_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT AVG(trip_duration) as average, percentile_approx(trip_duration, 0.5) as median, MIN(trip_duration) as minimum,MAX(trip_duration) as maximum , taxi_colour\n",
      "FROM combined_data\n",
      "GROUP BY taxi_colour\n"
     ]
    }
   ],
   "source": [
    "query = open('sql/4b_i.sql','r').read()\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------+-------+--------+-----------+\n",
      "|           average|median|minimum| maximum|taxi_colour|\n",
      "+------------------+------+-------+--------+-----------+\n",
      "|1264.4590899553366|   626|      1|  202989|      green|\n",
      "|1021.8073857437691|   670|      1|45466304|     yellow|\n",
      "+------------------+------+-------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. What was the average, median, minimum and maximum trip distance in km?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.createOrReplaceTempView(\"combined_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT AVG(trip_distance*1.60934) as average, percentile_approx(trip_distance*1.60934, 0.5) as median, MIN(trip_distance*1.60934) as minimum,MAX(trip_distance*1.60934) as maximum , taxi_colour\n",
      "FROM combined_data\n",
      "GROUP BY taxi_colour\n"
     ]
    }
   ],
   "source": [
    "query = open('sql/4b_ii.sql','r').read()\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+-------+------------------+-----------+\n",
      "|          average|            median|minimum|           maximum|taxi_colour|\n",
      "+-----------------+------------------+-------+------------------+-----------+\n",
      "|4.666716746319932|2.9129053079128266|    0.0|12883.861334091796|      green|\n",
      "|4.722008193182318| 2.591037423021793|    0.0|  304943.929100625|     yellow|\n",
      "+-----------------+------------------+-------+------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii. What was the average, median, minimum and maximum speed in km per hour?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.createOrReplaceTempView(\"combined_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT AVG(trip_distance*1.60934*3600/trip_duration) as average, percentile_approx(trip_distance*1.60934*3600/trip_duration, 0.5) as median, MIN(trip_distance*1.60934*3600/trip_duration) as minimum,MAX(trip_distance*1.60934*3600/trip_duration) as maximum , taxi_colour\n",
      "FROM combined_data\n",
      "GROUP BY taxi_colour\n"
     ]
    }
   ],
   "source": [
    "query = open('sql/4b_iii.sql','r').read()\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+-------+--------------------+-----------+\n",
      "|           average|            median|minimum|             maximum|taxi_colour|\n",
      "+------------------+------------------+-------+--------------------+-----------+\n",
      "| 22.55420227846247|17.746235862338864|    0.0|  194955.45644036864|      green|\n",
      "|21.121994010781364|15.872942045264047|    0.0|1.6385046936749998E7|     yellow|\n",
      "+------------------+------------------+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. What was the percentage of trips where the driver received tips?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.createOrReplaceTempView(\"combined_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(SELECT 'tip_received' as tip_received, count(*) as tip,\n",
      "count(*) * 100.0 / (select count(*) from combined_data) as tip_percent\n",
      "FROM combined_data\n",
      "WHERE tip_amount > 0)\n",
      "UNION\n",
      "(SELECT 'no_tip' as tip_received, count(*) as tip,\n",
      "count(*) * 100.0 / (select count(*) from combined_data) as tip_percent\n",
      "FROM combined_data\n",
      "WHERE tip_amount = 0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = open('sql/4c.sql','r').read()\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-----------------+\n",
      "|tip_received|      tip|      tip_percent|\n",
      "+------------+---------+-----------------+\n",
      "|      no_tip| 87592694|37.01547023695498|\n",
      "|tip_received|149043497|62.98373614602137|\n",
      "+------------+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d. For trips where the driver received tips, What was the percentage where the driver received tips of at least $10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.createOrReplaceTempView(\"combined_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(SELECT 'tip_over10' as tip_received, count(*) as tip,\n",
      "count(*) * 100.0 / (select count(*) from combined_data where tip_amount>0) as tip_percent\n",
      "FROM combined_data\n",
      "WHERE tip_amount >= 10)\n",
      "UNION\n",
      "(SELECT 'tip_under10' as tip_received, count(*) as tip,\n",
      "count(*) * 100.0 / (select count(*) from combined_data where tip_amount>0) as tip_percent\n",
      "FROM combined_data\n",
      "WHERE tip_amount < 10\n",
      "AND tip_amount>0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = open('sql/4d.sql','r').read()\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-----------------+\n",
      "|tip_received|      tip|      tip_percent|\n",
      "+------------+---------+-----------------+\n",
      "|  tip_over10|  4979240| 3.34079654612506|\n",
      "| tip_under10|144064257|96.65920345387494|\n",
      "+------------+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e. Classify each trip into bins of durations:\n",
    "* Under 5 Mins\n",
    "* From 5 mins to 10 mins\n",
    "* From 10 mins to 20 mins\n",
    "* From 20 mins to 30 mins\n",
    "* At least 30 mins\n",
    "### Then for each bins, calculate: \n",
    "* Average speed (km per hour)\n",
    "* Average distance per dollar (km per $)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.createOrReplaceTempView(\"combined_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(SELECT 'Under 5 Mins' as duration, AVG(trip_distance*1.60934*3600/trip_duration) as average\n",
      "FROM combined_data\n",
      "WHERE trip_duration < 60*5)\n",
      "UNION\n",
      "(SELECT '5-10 mins' as duration, AVG(trip_distance*1.60934*3600/trip_duration) as average\n",
      "FROM combined_data\n",
      "WHERE trip_duration >= 60*5\n",
      "AND trip_duration < 60*10)\n",
      "UNION\n",
      "(SELECT '10 to 20 mins' as duration, AVG(trip_distance*1.60934*3600/trip_duration) as average\n",
      "FROM combined_data\n",
      "WHERE trip_duration >= 60*10\n",
      "AND trip_duration < 60*20)\n",
      "UNION\n",
      "(SELECT '20 to 30 mins' as duration, AVG(trip_distance*1.60934*3600/trip_duration) as average\n",
      "FROM combined_data\n",
      "WHERE trip_duration >= 60*20\n",
      "AND trip_duration < 60*30)\n",
      "UNION\n",
      "(SELECT 'At least 30 mins' as duration, AVG(trip_distance*1.60934*3600/trip_duration) as average\n",
      "FROM combined_data\n",
      "WHERE trip_duration >= 60*30)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = open('sql/4e_i.sql','r').read()\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+\n",
      "|        duration|           average|\n",
      "+----------------+------------------+\n",
      "|       5-10 mins|16.498508623452828|\n",
      "|   20 to 30 mins| 20.46148926681209|\n",
      "|    Under 5 Mins| 38.65416235197025|\n",
      "|   10 to 20 mins|16.994521521610125|\n",
      "|At least 30 mins| 24.05022920905049|\n",
      "+----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average distance per dollar (km per $)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.createOrReplaceTempView(\"combined_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(SELECT 'Under 5 Mins' as duration, AVG(trip_distance*1.60934/total_amount) as km_per_dollar\n",
      "FROM combined_data\n",
      "WHERE trip_duration < 60*5)\n",
      "UNION\n",
      "(SELECT '5-10 mins' as duration, AVG(trip_distance*1.60934/total_amount) as  km_per_dollar\n",
      "FROM combined_data\n",
      "WHERE trip_duration >= 60*5\n",
      "AND trip_duration < 60*10)\n",
      "UNION\n",
      "(SELECT '10 to 20 mins' as duration, AVG(trip_distance*1.60934/total_amount) as  km_per_dollar\n",
      "FROM combined_data\n",
      "WHERE trip_duration >= 60*10\n",
      "AND trip_duration < 60*20)\n",
      "UNION\n",
      "(SELECT '20 to 30 mins' as duration, AVG(trip_distance*1.60934/total_amount) as  km_per_dollar\n",
      "FROM combined_data\n",
      "WHERE trip_duration >= 60*20\n",
      "AND trip_duration < 60*30)\n",
      "UNION\n",
      "(SELECT 'At least 30 mins' as duration, AVG(trip_distance*1.60934/total_amount) as  km_per_dollar\n",
      "FROM combined_data\n",
      "WHERE trip_duration >= 60*30)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = open('sql/4e_ii.sql','r').read()\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------------+\n",
      "|        duration|      km_per_dollar|\n",
      "+----------------+-------------------+\n",
      "|   10 to 20 mins|0.26022949571501425|\n",
      "|       5-10 mins| 0.2152663771114019|\n",
      "|At least 30 mins|0.38057057660315985|\n",
      "|   20 to 30 mins| 0.3074613587352144|\n",
      "|    Under 5 Mins|  0.170903336953278|\n",
      "+----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f. Which duration bin will you advise a taxi driver to target to maximise his income?\n",
    "\n",
    "Based on the above query, shorter trips have a income to distance ratio. However, this does not take into account that taxi's have wait time. A longer trips may be more profitable if wait times are typically long (i.e. time between fares), however, if times between fares are short - then shorter trips are more profitable.\n",
    "\n",
    "Assuming the wait times are approximately the same, to maximise the income, the driver should look at the fare with the highest hourly rate (total amount/hour). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.createOrReplaceTempView(\"combined_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(SELECT 'Under 5 Mins' as duration, AVG(total_amount*3600/trip_duration) as hourly_rate\n",
      "FROM combined_data\n",
      "WHERE trip_duration < 60*5)\n",
      "UNION\n",
      "(SELECT '5-10 mins' as duration, AVG(total_amount*3600/trip_duration) as hourly_rate\n",
      "FROM combined_data\n",
      "WHERE trip_duration >= 60*5\n",
      "AND trip_duration < 60*10)\n",
      "UNION\n",
      "(SELECT '10 to 20 mins' as duration, AVG(total_amount*3600/trip_duration) as hourly_rate\n",
      "FROM combined_data\n",
      "WHERE trip_duration >= 60*10\n",
      "AND trip_duration < 60*20)\n",
      "UNION\n",
      "(SELECT '20 to 30 mins' as duration, AVG(total_amount*3600/trip_duration) as hourly_rate\n",
      "FROM combined_data\n",
      "WHERE trip_duration >= 60*20\n",
      "AND trip_duration < 60*30)\n",
      "UNION\n",
      "(SELECT 'At least 30 mins' as duration, AVG(total_amount*3600/trip_duration) as hourly_rate\n",
      "FROM combined_data\n",
      "WHERE trip_duration >= 60*30)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = open('sql/4f.sql','r').read()\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+\n",
      "|        duration|      hourly_rate|\n",
      "+----------------+-----------------+\n",
      "|At least 30 mins|64.56451775422099|\n",
      "|       5-10 mins|76.67600165536875|\n",
      "|   20 to 30 mins|63.14220842975798|\n",
      "|   10 to 20 mins|63.13422234928635|\n",
      "|    Under 5 Mins|811.7968481049274|\n",
      "+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Train an ML Model to predict 'total_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from pyspark.sql.types import DateType, IntegerType, BooleanType, TimestampType, FloatType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import lit\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName('assignment_1') \\\n",
    "        .getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = spark.read.parquet('data/combined_clean_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+----------+------------+------------+---------------+-------------+-----+-------+----------+------------+---------------------+------------+------------+-----------+-------------------+-------------------+-------------+\n",
      "|VendorID|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|payment_type|taxi_colour|        pickup_date|       dropoff_date|trip_duration|\n",
      "+--------+------------------+----------+------------+------------+---------------+-------------+-----+-------+----------+------------+---------------------+------------+------------+-----------+-------------------+-------------------+-------------+\n",
      "|       2|             false|         1|         244|         116|              1|         1.77|  0.0|    0.5|       0.0|         0.0|                  0.3|         8.8|           2|     yellow|2017-03-13 09:09:07|2017-03-13 09:17:28|          501|\n",
      "|       1|             false|         1|         264|         264|              1|          1.5|  0.0|    0.5|       1.0|         0.0|                  0.3|        11.8|           1|     yellow|2017-03-13 09:09:08|2017-03-13 09:22:45|          817|\n",
      "|       1|             false|         1|         162|          88|              1|          5.7|  0.0|    0.5|       0.0|         0.0|                  0.3|        21.8|           1|     yellow|2017-03-13 09:09:08|2017-03-13 09:28:58|         1190|\n",
      "|       1|             false|         1|         233|         170|              1|          0.9|  0.0|    0.5|      1.75|         0.0|                  0.3|       10.55|           1|     yellow|2017-03-13 09:09:08|2017-03-13 09:20:03|          655|\n",
      "|       2|             false|         1|         164|         246|              5|         1.12|  0.0|    0.5|       0.0|         0.0|                  0.3|        11.3|           2|     yellow|2017-03-13 09:09:08|2017-03-13 09:24:19|          911|\n",
      "+--------+------------------+----------+------------+------------+---------------+-------------+-----+-------+----------+------------+---------------------+------------+------------+-----------+-------------------+-------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['RatecodeID','payment_type','taxi_colour','VendorID']\n",
    "cols = [\"VendorID\",\n",
    " \"RatecodeID\",\n",
    " \"passenger_count\",\n",
    " \"trip_distance\",\n",
    " \"extra\",\n",
    " \"mta_tax\",\n",
    " \"tip_amount\",\n",
    " \"tolls_amount\",\n",
    " \"improvement_surcharge\",\n",
    " \"total_amount\",\n",
    " \"payment_type\",\n",
    " \"taxi_colour\",\n",
    " \"pickup_date\",\n",
    " \"dropoff_date\",\n",
    "\"trip_duration\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dfs.select(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = []\n",
    "for cat_col in cat_cols:\n",
    "    col_indexer = StringIndexer(inputCol=cat_col, outputCol=f\"{cat_col}_ind\")\n",
    "    col_encoder = OneHotEncoderEstimator(inputCols=[f\"{cat_col}_ind\"], outputCols=[f\"{cat_col}_ohe\"])\n",
    "    stages += [col_indexer, col_encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [\n",
    " \"passenger_count\",\n",
    " \"trip_distance\",\n",
    " \"extra\",\n",
    " \"mta_tax\",\n",
    " \"tip_amount\",\n",
    " \"tolls_amount\",\n",
    " \"improvement_surcharge\",\n",
    " \"trip_duration\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols_ohe = [f\"{cat_col}_ohe\" for cat_col in cat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=cat_cols_ohe + num_cols, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1159, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 985, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1164, in send_command\n",
      "    \"Error while receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while receiving\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38155)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-27-af417fac4d8b>\", line 1, in <module>\n",
      "    pipeline_model = pipeline.fit(df)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/pipeline.py\", line 112, in _fit\n",
      "    dataset = model.transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 173, in transform\n",
      "    return self._transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 312, in _transform\n",
      "    return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sql_ctx)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling o113.transform\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38155)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-27-af417fac4d8b>\", line 1, in <module>\n",
      "    pipeline_model = pipeline.fit(df)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/pipeline.py\", line 112, in _fit\n",
      "    dataset = model.transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 173, in transform\n",
      "    return self._transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 312, in _transform\n",
      "    return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sql_ctx)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling o113.transform\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38155)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-27-af417fac4d8b>\", line 1, in <module>\n",
      "    pipeline_model = pipeline.fit(df)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/pipeline.py\", line 112, in _fit\n",
      "    dataset = model.transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 173, in transform\n",
      "    return self._transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 312, in _transform\n",
      "    return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sql_ctx)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling o113.transform\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38155)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-27-af417fac4d8b>\", line 1, in <module>\n",
      "    pipeline_model = pipeline.fit(df)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/pipeline.py\", line 112, in _fit\n",
      "    dataset = model.transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 173, in transform\n",
      "    return self._transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 312, in _transform\n",
      "    return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sql_ctx)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling o113.transform\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38155)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-27-af417fac4d8b>\", line 1, in <module>\n",
      "    pipeline_model = pipeline.fit(df)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/pipeline.py\", line 112, in _fit\n",
      "    dataset = model.transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 173, in transform\n",
      "    return self._transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 312, in _transform\n",
      "    return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sql_ctx)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling o113.transform\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38155)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-27-af417fac4d8b>\", line 1, in <module>\n",
      "    pipeline_model = pipeline.fit(df)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/pipeline.py\", line 112, in _fit\n",
      "    dataset = model.transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 173, in transform\n",
      "    return self._transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 312, in _transform\n",
      "    return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sql_ctx)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling o113.transform\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38155)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-27-af417fac4d8b>\", line 1, in <module>\n",
      "    pipeline_model = pipeline.fit(df)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/pipeline.py\", line 112, in _fit\n",
      "    dataset = model.transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 173, in transform\n",
      "    return self._transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 312, in _transform\n",
      "    return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sql_ctx)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling o113.transform\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38155)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-27-af417fac4d8b>\", line 1, in <module>\n",
      "    pipeline_model = pipeline.fit(df)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/pipeline.py\", line 112, in _fit\n",
      "    dataset = model.transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 173, in transform\n",
      "    return self._transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 312, in _transform\n",
      "    return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sql_ctx)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling o113.transform\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38155)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-27-af417fac4d8b>\", line 1, in <module>\n",
      "    pipeline_model = pipeline.fit(df)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/pipeline.py\", line 112, in _fit\n",
      "    dataset = model.transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 173, in transform\n",
      "    return self._transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 312, in _transform\n",
      "    return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sql_ctx)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling o113.transform\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38155)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-27-af417fac4d8b>\", line 1, in <module>\n",
      "    pipeline_model = pipeline.fit(df)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/pipeline.py\", line 112, in _fit\n",
      "    dataset = model.transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 173, in transform\n",
      "    return self._transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 312, in _transform\n",
      "    return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sql_ctx)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling o113.transform\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38155)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-27-af417fac4d8b>\", line 1, in <module>\n",
      "    pipeline_model = pipeline.fit(df)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/pipeline.py\", line 112, in _fit\n",
      "    dataset = model.transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 173, in transform\n",
      "    return self._transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 312, in _transform\n",
      "    return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sql_ctx)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling o113.transform\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38155)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-27-af417fac4d8b>\", line 1, in <module>\n",
      "    pipeline_model = pipeline.fit(df)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/pipeline.py\", line 112, in _fit\n",
      "    dataset = model.transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 173, in transform\n",
      "    return self._transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 312, in _transform\n",
      "    return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sql_ctx)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling o113.transform\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38155)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-27-af417fac4d8b>\", line 1, in <module>\n",
      "    pipeline_model = pipeline.fit(df)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/pipeline.py\", line 112, in _fit\n",
      "    dataset = model.transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 173, in transform\n",
      "    return self._transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 312, in _transform\n",
      "    return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sql_ctx)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling o113.transform\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38155)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-27-af417fac4d8b>\", line 1, in <module>\n",
      "    pipeline_model = pipeline.fit(df)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/pipeline.py\", line 112, in _fit\n",
      "    dataset = model.transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 173, in transform\n",
      "    return self._transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 312, in _transform\n",
      "    return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sql_ctx)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling o113.transform\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38155)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-27-af417fac4d8b>\", line 1, in <module>\n",
      "    pipeline_model = pipeline.fit(df)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/pipeline.py\", line 112, in _fit\n",
      "    dataset = model.transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 173, in transform\n",
      "    return self._transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 312, in _transform\n",
      "    return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sql_ctx)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling o113.transform\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38155)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-27-af417fac4d8b>\", line 1, in <module>\n",
      "    pipeline_model = pipeline.fit(df)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/pipeline.py\", line 112, in _fit\n",
      "    dataset = model.transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 173, in transform\n",
      "    return self._transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 312, in _transform\n",
      "    return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sql_ctx)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling o113.transform\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38155)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-27-af417fac4d8b>\", line 1, in <module>\n",
      "    pipeline_model = pipeline.fit(df)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/pipeline.py\", line 112, in _fit\n",
      "    dataset = model.transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 173, in transform\n",
      "    return self._transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 312, in _transform\n",
      "    return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sql_ctx)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling o113.transform\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38155)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-27-af417fac4d8b>\", line 1, in <module>\n",
      "    pipeline_model = pipeline.fit(df)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/pipeline.py\", line 112, in _fit\n",
      "    dataset = model.transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 173, in transform\n",
      "    return self._transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 312, in _transform\n",
      "    return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sql_ctx)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling o113.transform\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38155)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-27-af417fac4d8b>\", line 1, in <module>\n",
      "    pipeline_model = pipeline.fit(df)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/pipeline.py\", line 112, in _fit\n",
      "    dataset = model.transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 173, in transform\n",
      "    return self._transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 312, in _transform\n",
      "    return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sql_ctx)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling o113.transform\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38155)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-27-af417fac4d8b>\", line 1, in <module>\n",
      "    pipeline_model = pipeline.fit(df)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/pipeline.py\", line 112, in _fit\n",
      "    dataset = model.transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 173, in transform\n",
      "    return self._transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 312, in _transform\n",
      "    return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sql_ctx)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling o113.transform\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38155)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-27-af417fac4d8b>\", line 1, in <module>\n",
      "    pipeline_model = pipeline.fit(df)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/pipeline.py\", line 112, in _fit\n",
      "    dataset = model.transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 173, in transform\n",
      "    return self._transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 312, in _transform\n",
      "    return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sql_ctx)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling o113.transform\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38155)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-27-af417fac4d8b>\", line 1, in <module>\n",
      "    pipeline_model = pipeline.fit(df)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/pipeline.py\", line 112, in _fit\n",
      "    dataset = model.transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 173, in transform\n",
      "    return self._transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 312, in _transform\n",
      "    return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sql_ctx)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling o113.transform\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38155)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-27-af417fac4d8b>\", line 1, in <module>\n",
      "    pipeline_model = pipeline.fit(df)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/pipeline.py\", line 112, in _fit\n",
      "    dataset = model.transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 173, in transform\n",
      "    return self._transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 312, in _transform\n",
      "    return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sql_ctx)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling o113.transform\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38155)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-27-af417fac4d8b>\", line 1, in <module>\n",
      "    pipeline_model = pipeline.fit(df)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/pipeline.py\", line 112, in _fit\n",
      "    dataset = model.transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 173, in transform\n",
      "    return self._transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 312, in _transform\n",
      "    return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sql_ctx)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling o113.transform\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38155)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-27-af417fac4d8b>\", line 1, in <module>\n",
      "    pipeline_model = pipeline.fit(df)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/pipeline.py\", line 112, in _fit\n",
      "    dataset = model.transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 173, in transform\n",
      "    return self._transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 312, in _transform\n",
      "    return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sql_ctx)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling o113.transform\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38155)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-27-af417fac4d8b>\", line 1, in <module>\n",
      "    pipeline_model = pipeline.fit(df)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/pipeline.py\", line 112, in _fit\n",
      "    dataset = model.transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 173, in transform\n",
      "    return self._transform(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 312, in _transform\n",
      "    return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sql_ctx)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling o113.transform\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o113.transform",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-af417fac4d8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipeline_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    110\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    334\u001b[0m             raise Py4JError(\n\u001b[1;32m    335\u001b[0m                 \u001b[0;34m\"An error occurred while calling {0}{1}{2}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 format(target_id, \".\", name))\n\u001b[0m\u001b[1;32m    337\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o113.transform"
     ]
    }
   ],
   "source": [
    "pipeline_model = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pipeline_model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.filter(F.col(\"pickup_date\")<\"2018-10-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------------+-------------+-----+-------+----------+------------+---------------------+------------+------------+-----------+-------------------+-------------------+-------------+--------------+--------------+----------------+----------------+---------------+---------------+------------+-------------+--------------------+\n",
      "|VendorID|RatecodeID|passenger_count|trip_distance|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|payment_type|taxi_colour|        pickup_date|       dropoff_date|trip_duration|RatecodeID_ind|RatecodeID_ohe|payment_type_ind|payment_type_ohe|taxi_colour_ind|taxi_colour_ohe|VendorID_ind| VendorID_ohe|            features|\n",
      "+--------+----------+---------------+-------------+-----+-------+----------+------------+---------------------+------------+------------+-----------+-------------------+-------------------+-------------+--------------+--------------+----------------+----------------+---------------+---------------+------------+-------------+--------------------+\n",
      "|       2|         1|              1|         1.77|  0.0|    0.5|       0.0|         0.0|                  0.3|         8.8|           2|     yellow|2017-03-13 09:09:07|2017-03-13 09:17:28|          501|           0.0| (6,[0],[1.0])|             1.0|   (4,[1],[1.0])|            0.0|  (1,[0],[1.0])|         0.0|(2,[0],[1.0])|(21,[0,7,10,11,13...|\n",
      "|       1|         1|              1|          1.5|  0.0|    0.5|       1.0|         0.0|                  0.3|        11.8|           1|     yellow|2017-03-13 09:09:08|2017-03-13 09:22:45|          817|           0.0| (6,[0],[1.0])|             0.0|   (4,[0],[1.0])|            0.0|  (1,[0],[1.0])|         1.0|(2,[1],[1.0])|(21,[0,6,10,12,13...|\n",
      "|       1|         1|              1|          5.7|  0.0|    0.5|       0.0|         0.0|                  0.3|        21.8|           1|     yellow|2017-03-13 09:09:08|2017-03-13 09:28:58|         1190|           0.0| (6,[0],[1.0])|             0.0|   (4,[0],[1.0])|            0.0|  (1,[0],[1.0])|         1.0|(2,[1],[1.0])|(21,[0,6,10,12,13...|\n",
      "|       1|         1|              1|          0.9|  0.0|    0.5|      1.75|         0.0|                  0.3|       10.55|           1|     yellow|2017-03-13 09:09:08|2017-03-13 09:20:03|          655|           0.0| (6,[0],[1.0])|             0.0|   (4,[0],[1.0])|            0.0|  (1,[0],[1.0])|         1.0|(2,[1],[1.0])|(21,[0,6,10,12,13...|\n",
      "|       2|         1|              5|         1.12|  0.0|    0.5|       0.0|         0.0|                  0.3|        11.3|           2|     yellow|2017-03-13 09:09:08|2017-03-13 09:24:19|          911|           0.0| (6,[0],[1.0])|             1.0|   (4,[1],[1.0])|            0.0|  (1,[0],[1.0])|         0.0|(2,[0],[1.0])|(21,[0,7,10,11,13...|\n",
      "+--------+----------+---------------+-------------+-----+-------+----------+------------+---------------------+------------+------------+-----------+-------------------+-------------------+-------------+--------------+--------------+----------------+----------------+---------------+---------------+------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df.filter(F.col(\"pickup_date\")>=\"2018-10-01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(featuresCol='features', labelCol='total_amount', numTrees=1, maxDepth=10, minInstancesPerNode=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = rf.fit(df_train)\n",
    "rf_model.save(\"models/rf_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

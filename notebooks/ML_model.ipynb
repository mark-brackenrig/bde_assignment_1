{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DateType, IntegerType, BooleanType, FloatType, NumericType\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work/notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName('assignment_1') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('data/combined_cleaned_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- store_and_fwd_flag: boolean (nullable = true)\n",
      " |-- RatecodeID: string (nullable = true)\n",
      " |-- PULocationID: string (nullable = true)\n",
      " |-- DOLocationID: string (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: float (nullable = true)\n",
      " |-- extra: float (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: float (nullable = true)\n",
      " |-- tolls_amount: float (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- taxi_colour: string (nullable = true)\n",
      " |-- pickup_date: timestamp (nullable = true)\n",
      " |-- dropoff_date: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType, IntegerType, BooleanType, FloatType\n",
    "df= df.withColumn('total_amount', F.col('total_amount').astype(FloatType())).\\\n",
    "    withColumn('mta_tax', F.col('mta_tax').astype(FloatType())).\\\n",
    "    withColumn('improvement_surcharge', F.col('improvement_surcharge').astype(FloatType()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['RatecodeID','payment_type','taxi_colour','VendorID']\n",
    "cols = [\"VendorID\",\n",
    " \"RatecodeID\",\n",
    " \"passenger_count\",\n",
    " \"trip_distance\",\n",
    " \"extra\",\n",
    " \"mta_tax\",\n",
    " \"tip_amount\",\n",
    " \"tolls_amount\",\n",
    " \"improvement_surcharge\",\n",
    " \"total_amount\",\n",
    " \"payment_type\",\n",
    " \"taxi_colour\",\n",
    " \"pickup_date\",\n",
    " \"dropoff_date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = []\n",
    "for cat_col in cat_cols:\n",
    "    col_indexer = StringIndexer(inputCol=cat_col, outputCol=f\"{cat_col}_ind\")\n",
    "    col_encoder = OneHotEncoderEstimator(inputCols=[f\"{cat_col}_ind\"], outputCols=[f\"{cat_col}_ohe\"])\n",
    "    stages += [col_indexer, col_encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('trip_duration',F.col(\"dropoff_date\").cast(\"long\") - F.col('pickup_date').cast(\"long\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [\n",
    " \"passenger_count\",\n",
    " \"trip_distance\",\n",
    " \"extra\",\n",
    " \"mta_tax\",\n",
    " \"tip_amount\",\n",
    " \"tolls_amount\",\n",
    " \"improvement_surcharge\",\n",
    " \"trip_duration\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols_ohe = [f\"{cat_col}_ohe\" for cat_col in cat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=cat_cols_ohe + num_cols, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into Training and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pipeline_model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.filter(F.col(\"pickup_date\")<\"2018-10-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------------+-------------+-----+-------+----------+------------+---------------------+------------+------------+-----------+-------------------+-------------------+-------------+--------------+--------------+----------------+----------------+---------------+---------------+------------+-------------+--------------------+\n",
      "|VendorID|RatecodeID|passenger_count|trip_distance|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|payment_type|taxi_colour|        pickup_date|       dropoff_date|trip_duration|RatecodeID_ind|RatecodeID_ohe|payment_type_ind|payment_type_ohe|taxi_colour_ind|taxi_colour_ohe|VendorID_ind| VendorID_ohe|            features|\n",
      "+--------+----------+---------------+-------------+-----+-------+----------+------------+---------------------+------------+------------+-----------+-------------------+-------------------+-------------+--------------+--------------+----------------+----------------+---------------+---------------+------------+-------------+--------------------+\n",
      "|       2|         1|              5|         0.51|  0.5|    0.5|       0.7|         0.0|                  0.3|         6.0|           1|      green|2018-06-01 00:33:55|2018-06-01 00:36:13|          138|           0.0| (6,[0],[1.0])|             0.0|   (4,[0],[1.0])|            1.0|      (1,[],[])|         0.0|(2,[0],[1.0])|(21,[0,6,11,13,14...|\n",
      "|       2|         1|              5|         1.97|  0.5|    0.5|      2.06|         0.0|                  0.3|       12.36|           1|      green|2018-06-01 00:40:36|2018-06-01 00:49:46|          550|           0.0| (6,[0],[1.0])|             0.0|   (4,[0],[1.0])|            1.0|      (1,[],[])|         0.0|(2,[0],[1.0])|(21,[0,6,11,13,14...|\n",
      "|       2|         1|              5|          1.4|  0.5|    0.5|       0.0|         0.0|                  0.3|         7.8|           2|      green|2018-06-01 00:57:12|2018-06-01 01:02:58|          346|           0.0| (6,[0],[1.0])|             1.0|   (4,[1],[1.0])|            1.0|      (1,[],[])|         0.0|(2,[0],[1.0])|(21,[0,7,11,13,14...|\n",
      "|       2|         1|              1|         1.36|  0.5|    0.5|       0.0|         0.0|                  0.3|         8.3|           2|      green|2018-06-01 00:10:13|2018-06-01 00:16:27|          374|           0.0| (6,[0],[1.0])|             1.0|   (4,[1],[1.0])|            1.0|      (1,[],[])|         0.0|(2,[0],[1.0])|(21,[0,7,11,13,14...|\n",
      "|       1|         1|              1|          7.9|  0.5|    0.5|       6.3|         0.0|                  0.3|        31.6|           1|      green|2018-06-01 00:32:08|2018-06-01 00:52:06|         1198|           0.0| (6,[0],[1.0])|             0.0|   (4,[0],[1.0])|            1.0|      (1,[],[])|         1.0|(2,[1],[1.0])|(21,[0,6,12,13,14...|\n",
      "+--------+----------+---------------+-------------+-----+-------+----------+------------+---------------------+------------+------------+-----------+-------------------+-------------------+-------------+--------------+--------------+----------------+----------------+---------------+---------------+------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df.filter(F.col(\"pickup_date\")>=\"2018-10-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(featuresCol='features', labelCol='total_amount', numTrees=100, maxDepth=15, minInstancesPerNode=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = rf.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.save(\"models/rf_basic\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

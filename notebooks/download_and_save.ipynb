{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work/notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " '.env',\n",
       " '.git',\n",
       " '.gitattributes',\n",
       " '.ipynb_checkpoints',\n",
       " 'Dockerfile',\n",
       " 'README.md',\n",
       " 'Untitled.ipynb',\n",
       " 'data',\n",
       " 'docker-compose.yml',\n",
       " 'error_logs',\n",
       " 'hs_err_pid133.log',\n",
       " 'hs_err_pid277.log',\n",
       " 'hs_err_pid681.log',\n",
       " 'notebooks',\n",
       " 'spark-warehouse',\n",
       " 'src']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset\n",
    "\n",
    "First step is to download the dataset from S3 - to do this, I used the boto3 library to access the public S3 data. I will be working with the 2017-18 data in this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define function to download data and save it\n",
    "\n",
    "Not sure if I'll need this with Spark - might still be worthwhile having data stored locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_save_data(bucket = 'nyc-tlc',prefix = 'trip data',year_regex = '201[78]', data_folder = 'data'):\n",
    "    \"\"\"\n",
    "    This function downloads and saves relevant data from the S3 bucket to my local machine.\n",
    "    \n",
    "    params:\n",
    "    * s3 - a boto3.client('s3') object with relevant permissions\n",
    "    * bucket - bucket of stored data\n",
    "    * prefix - folder with relevant stored data\n",
    "    * year_regex - the years for the project\n",
    "    * data_folder - where I want to store the data\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    print(\"Starting yellow cabs download\")\n",
    "    sys.stdout.write(\"[%s]\" % (\" \" * 12))\n",
    "    sys.stdout.flush()\n",
    "    sys.stdout.write(\"\\b\" * (12+1)) # return to start of line, after '['\n",
    "    contents = s3.list_objects(Bucket = bucket, Prefix = prefix)['Contents']\n",
    "    #retrieve yellow cab keys and download file\n",
    "    yellow_cab_keys = [i['Key'] for i in contents if ('yellow_tripdata' in i['Key'])&(bool(re.search(year_regex,i['Key'])))]\n",
    "    for i in yellow_cab_keys:\n",
    "        if (re.sub(\".*/\",\"\",i) in os.listdir(data_folder+'/yellow_cabs/'))==False:\n",
    "            s3.download_file('nyc-tlc',i,data_folder+'/yellow_cabs/'+re.sub(\".*/\",\"\",i)) \n",
    "        sys.stdout.write(\"-\")\n",
    "        sys.stdout.flush()\n",
    "    sys.stdout.write(\"]\\n\")\n",
    "    print('Yellow Cabs completed')\n",
    "    print(\"Starting green cabs download\")\n",
    "    sys.stdout.write(\"[%s]\" % (\" \" * 12))\n",
    "    sys.stdout.flush()\n",
    "    sys.stdout.write(\"\\b\" * (12+1)) # return to start of line, after '['\n",
    "    green_cab_keys = [i['Key'] for i in contents if ('green_tripdata' in i['Key'])&(bool(re.search(year_regex,i['Key'])))]\n",
    "    for i in green_cab_keys:\n",
    "        if (re.sub(\".*/\",\"\",i) in os.listdir(data_folder+'/green_cabs/'))==False:\n",
    "            s3.download_file('nyc-tlc',i,data_folder+'/green_cabs/'+re.sub(\".*/\",\"\",i)) \n",
    "        sys.stdout.write(\"-\")\n",
    "        sys.stdout.flush()\n",
    "    sys.stdout.write(\"]\\n\")\n",
    "    print(\"Green Cabs completed\")\n",
    "    return \"Data downloaded and saved\"\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting yellow cabs download\n",
      "[            ------------------------]\n",
      "Yellow Cabs completed\n",
      "Starting green cabs download\n",
      "[            ------------------------]\n",
      "Green Cabs completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Data downloaded and saved'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_and_save_data(bucket = 'nyc-tlc',prefix = 'trip data',year_regex = '201[78]', data_folder = 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName('assignment_1') \\\n",
    "        .getOrCreate() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.5'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.0'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark._jvm.org.apache.hadoop.util.VersionInfo.getVersion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = [\"data/green_cabs/\" + i for i in  os.listdir('data/green_cabs')]\n",
    "y = [\"data/yellow_cabs/\" + i for i in  os.listdir('data/yellow_cabs')]\n",
    "paths = [i for y in [g,y] for i in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green = spark.read.format('csv').options(header ='true').load(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow = spark.read.format('csv').options(header ='true').load(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a variable for the colour of the taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "green = green.withColumn('taxi_colour', lit('green'))\n",
    "yellow = yellow.withColumn('taxi_colour', lit('yellow'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the schemas - Note that there is a difference in column names and types. We need to cre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType, IntegerType, BooleanType, TimestampType, FloatType\n",
    "green = green.withColumn('pickup_date', F.col('lpep_pickup_datetime').astype(TimestampType())).\\\n",
    "    withColumn(\"dropoff_date\", F.col(\"lpep_dropoff_datetime\").astype(TimestampType()))\n",
    "yellow = yellow.withColumn('pickup_date', F.col('tpep_pickup_datetime').astype(TimestampType()) ).\\\n",
    "    withColumn(\"dropoff_date\", F.col(\"tpep_dropoff_datetime\").astype(TimestampType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols =['VendorID',\n",
    " 'store_and_fwd_flag',\n",
    " 'RatecodeID',\n",
    " 'PULocationID',\n",
    " 'DOLocationID',\n",
    " 'passenger_count',\n",
    " 'trip_distance',\n",
    " 'extra',\n",
    " 'mta_tax',\n",
    " 'tip_amount',\n",
    " 'tolls_amount',\n",
    " 'improvement_surcharge',\n",
    " 'total_amount',\n",
    " 'payment_type',\n",
    " 'taxi_colour',\n",
    " 'pickup_date',\n",
    " 'dropoff_date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removed columns\n",
    "\n",
    "Some columns have been removed from the dataset. Namely:\n",
    "1. ehail_fee:  This is not in the data dictionary, but assume it is the fee for a cab to be 'hailed'. It does not exist in the yellow cabs dataset so therefore it should be excluded\n",
    "2. Trip_type: only exists in the green taxi dataset - refers to whether a cab was dispatched or hailed\n",
    "3. date/time stamps: these were transformed into new columns and had names aligned.\n",
    "4. fair_amount: removed as per assignment brief\n",
    "\n",
    "I have decided to leave 'ID' fields as strings for now, as they do not represent numbers but rather a categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green = green.select(cols)\n",
    "yellow = yellow.select(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = green.union(yellow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert numeric to correct datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType, IntegerType, BooleanType, FloatType\n",
    "dfs= dfs.withColumn('store_and_fwd_flag', F.col('store_and_fwd_flag').astype(BooleanType())).\\\n",
    "    withColumn('passenger_count', F.col('passenger_count').astype(IntegerType())).\\\n",
    "    withColumn('trip_distance', F.col('trip_distance').astype(FloatType())).\\\n",
    "    withColumn('extra', F.col('extra').astype(FloatType())).\\\n",
    "    withColumn('tip_amount', F.col('tip_amount').astype(FloatType())).\\\n",
    "    withColumn('tolls_amount', F.col('tolls_amount').astype(FloatType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = dfs.withColumn('trip_duration',F.col(\"dropoff_date\").cast(\"long\") - F.col('pickup_date').cast(\"long\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are a small number of records that appear to be errornous - due to their small size, I believe they should be excluded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_cleaned = dfs.where(\"YEAR(pickup_date) <2019 AND YEAR(pickup_date) >2016 AND trip_duration > 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_cleaned.write.mode('overwrite').parquet('combined_cleaned_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read = spark.read.parquet('combined_cleaned_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
